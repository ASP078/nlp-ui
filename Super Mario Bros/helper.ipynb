{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nes_py.wrappers import JoypadSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_super_mario_bros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Definitions\n",
    "Environment The world that an agent interacts with and learns from.\n",
    "\n",
    "### Action ```a``` : \n",
    "How the Agent responds to the Environment. The set of all possible Actions is called action-space.\n",
    "\n",
    "### State ```s``` : \n",
    "The current characteristic of the Environment. The set of all possible States the Environment can be in is called state-space.\n",
    "\n",
    "### Reward ```r``` : \n",
    "Reward is the key feedback from Environment to Agent. It is what drives the Agent to learn and to change its future action. An aggregation of rewards over multiple time steps is called Return.\n",
    "\n",
    "### Optimal Action-Value function \n",
    "```latex\n",
    "Q*(s,a)\n",
    "``` \n",
    "Gives the expected return if you start in state ss, take an arbitrary action aa, and then for each future time step take the action that maximizes returns. Q can be said to stand for the “quality” of the action in a state. We try to approximate this function.\n",
    "\n",
    "### Environment\n",
    "Initialize Environment\n",
    "In Mario, the environment consists of tubes, mushrooms and other components.\n",
    "\n",
    "When Mario makes an action, the environment responds with the changed (next) state, reward and other info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make(\"SuperMarioBros-v0\")\n",
    "\n",
    "actions = [['right'],['right','A']]\n",
    "env = JoypadSpace(env, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next-State shape: (240, 256, 3), \n",
      " Reward: 0, \n",
      " Completed Stage: False, \n",
      " Information: {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "next_state, reward, done, info = env.step(action=0)\n",
    "\n",
    "print(f\"Next-State shape: {next_state.shape}, \\n Reward: {reward}, \\n Completed Stage: {done}, \\n Information: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Environment\n",
    "Environment data is returned to the agent in next_state. As you saw above, each state is represented by a [3, 240, 256] size array. Often that is more information than our agent needs; for instance, Mario’s actions do not depend on the color of the pipes or the sky!\n",
    "\n",
    "We use Wrappers to preprocess environment data before sending it to the agent.\n",
    "\n",
    "```GrayScaleObservation``` is a common wrapper to transform an RGB image to grayscale; doing so reduces the size of the state representation without losing useful information. Now the size of each state: [1, 240, 256]\n",
    "\n",
    "```ResizeObservation``` downsamples each observation into a square image. New size: [1, 84, 84]\n",
    "\n",
    "```SkipFrame``` is a custom wrapper that inherits from ```gym.Wrapper``` and implements the ```step()``` function. Because consecutive frames don’t vary much, we can skip n-intermediate frames without losing much information. The n-th frame aggregates rewards accumulated over each skipped frame.\n",
    "\n",
    "```FrameStack``` is a wrapper that allows us to squash consecutive frames of the environment into a single observation point to feed to our learning model. This way, we can identify if Mario was landing or jumping based on the direction of his movement in the previous several frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, n_skip):\n",
    "        \"\"\"\n",
    "            Return's only every n_skip'th frame\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = n_skip\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "            Repeare action and sum reward\n",
    "        \"\"\"\n",
    "\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # accumulate reward and repeate the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward+=reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "    \n",
    "    def permute_orientation(self, observation):\n",
    "        # permute the [H,W,C] array to [C,H,W] tensor\n",
    "\n",
    "        observation = np.transpose(observation, (2,0,1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = transforms.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "        \n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        transform = transforms.Compose(\n",
    "            [transforms.Resize(self.shape),\n",
    "            transforms.Normalize(0,255)]\n",
    "        )\n",
    "        observation = transform(observation).squeeze(0)\n",
    "\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SkipFrame(env, n_skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "We create a class Mario to represent our agent in the game. Mario should be able to:\n",
    "<ul>\n",
    "<li> <b>Act</b> according to the optimal action policy based on the current state (of the environment).\n",
    "<li> <b>Remember</b> experiences. Experience = (current state, current action, reward, next state). Mario caches and later recalls his experiences to update his action policy.\n",
    "<li> <b>Learn</b> a better action policy over time\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act\n",
    "For any given state, an agent can choose to do the most optimal action (exploit) or a random action (explore).\n",
    "\n",
    "Mario randomly explores with a chance of self.exploration_rate; when he chooses to exploit, he relies on MarioNet (implemented in Learn section) to provide the most optimal action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache and Recall\n",
    "These two functions serve as Mario’s “memory” process.\n",
    "\n",
    "```cache()```: Each time Mario performs an action, he stores the experience to his memory. His experience includes the current state, action performed, reward from the action, the next state, and whether the game is done.\n",
    "\n",
    "```recall()```: Mario randomly samples a batch of experiences from his memory, and uses that to learn the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn\n",
    "Mario uses the DDQN algorithm under the hood. DDQN uses two ConvNets - $ Q_{online} $ and $ Q_{target} $- that independently approximate the optimal action-value function.\n",
    "\n",
    "In our implementation, we share feature generator features across $ Q_{online} $ and $ Q_{target} $, but maintain separate FC classifiers for each. $ \\theta_{target} $(the parameters of $ Q_{target} $) is frozen to prevent updation by backprop. Instead, it is periodically synced with $ \\theta_{online} $(more on this later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD Estimate & TD Target\n",
    "Two values are involved in learning:\n",
    "\n",
    "TD Estimate - the predicted optimal $ Q^* $ for a given state ```s``` \n",
    "$ {TD}_e = Q_{online}^*(s,a) $\n",
    "\n",
    "TD Target - aggregation of current reward and the estimated $ Q^* $ in the next state ```s'``` \n",
    "\n",
    "$ a' = argmax_{a} Q_{online}(s', a) $\n",
    "\n",
    "$ {TD}_t = r + \\gamma Q_{target}^*(s',a') $\n",
    "\n",
    "Because we don’t know what next action ```a'``` will be, we use the action ```a'``` maximizes $ Q_{online} $ in the next state ```s'.```\n",
    "Notice we use the ```@torch.no_grad()``` decorator on ```td_target()``` to disable gradient calculations here (because we don’t need to backpropagate on $ \\theta_{target} $)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the model\n",
    "As Mario samples inputs from his replay buffer, we compute $TD_t$ and $TD_e$ and backpropagate this loss down $Q_{online}$ to update its parameters $ \\theta_{online} $ ($ \\alpha $ is the learning rate ```lr``` passed to the optimizer)\n",
    "\n",
    "$ \\theta_{online} \\leftarrow \\theta_{online} + \\alpha \\nabla(TD_e - TD_t) $\n",
    "\n",
    "$ \\theta_{target} $ does not update through backpropagation. Instead, we periodically copy $ \\theta_{online} $ to $ \\theta_{target} $\n",
    "\n",
    "$ \\theta_{target} \\leftarrow \\theta_{online} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "        # Mario's DNN to predict the most optimal action \n",
    "\n",
    "        self.online_net = MarioOnlineNet(self.state_dim, self.action_dim).float()\n",
    "        self.target_net = MarioTargetNet(self.state_dim, self.action_dim).float()\n",
    "\n",
    "        if self.use_cuda:\n",
    "            self.online_net = self.online_net.to(device='cuda')\n",
    "            self.target_net = self.target_net.to(device='cuda')\n",
    "        \n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        # number of experiences between saving Mario Net \n",
    "        self.save_every = 5e5\n",
    "\n",
    "        # replay buffer\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 32\n",
    "\n",
    "        # Q-Learning params\n",
    "        self.gamma = 0.9\n",
    "\n",
    "        # backpropagation params\n",
    "        self.optimizer = torch.optim.Adam(self.online_net.parameters(), lr=0.0025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "        # learning params\n",
    "        self.burnin = 1e4 # minimum experiences before training\n",
    "        self.learn_every = 3 # number of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4 # number experiences betweem Q_online and Q_target sync\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "            Given a state, choose an epsilon-greedy action and update value of step.\n",
    "\n",
    "            Inputs:\n",
    "                state(LazyFrame): A single observation of the current state, dimension is (state_dim)\n",
    "            Outputs:\n",
    "                action_idx(int): An integer representing which action Mario will perform\n",
    "        \"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "        \n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = state.__array__()\n",
    "            if self.use_cuda:\n",
    "                state = torch.tensor(state).cuda()\n",
    "            else:\n",
    "                state = torch.tensor(state)\n",
    "            \n",
    "            state = state.unsqueeze(0)\n",
    "            action_values = self.online_net(state)\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "        \n",
    "        # decrease exploration rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate, self.exploration_rate_min)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step+=1\n",
    "\n",
    "        return action_idx\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "            Stores the experience to self.memory (replay buffer)\n",
    "\n",
    "            Inputs:\n",
    "                state(LazyFrame)\n",
    "                next_step(LazyFrame)\n",
    "                action(int)\n",
    "                reward(float)\n",
    "                done(bool)\n",
    "        \"\"\"\n",
    "        state = state.__array__()\n",
    "        next_state = next_state.__array__()\n",
    "\n",
    "        if self.use_cuda:\n",
    "            state = torch.tensor(state).cuda()\n",
    "            next_state = torch.tensor(next_state).cuda()\n",
    "            action = torch.tensor([action]).cuda()\n",
    "            reward = torch.tensor([reward]).cuda()\n",
    "            done = torch.tensor([done]).cuda()\n",
    "        else:\n",
    "            state = torch.tensor(state)\n",
    "            next_state = torch.tensor(next_state)\n",
    "            action = torch.tensor([action])\n",
    "            reward = torch.tensor([reward])\n",
    "            done = torch.tensor([done])\n",
    "        \n",
    "        self.memory.append((\n",
    "            state,\n",
    "            next_state,\n",
    "            action,\n",
    "            reward,\n",
    "            done\n",
    "        ))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "            Retrive a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "            Update online action value (Q) function with a batch of experiences\n",
    "        \"\"\"\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "        \n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "        \n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "        \n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "        \n",
    "        # sample from the replay memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # get TD estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # get TD target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)\n",
    "        \n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        # Q_online(s,a)\n",
    "        current_Q = self.online_net(state)[np.arange(0, self.batch_size), action]\n",
    "\n",
    "        return current_Q\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "\n",
    "        next_state_Q = self.target_net(next_state)\n",
    "        # a'\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "\n",
    "        # Q_target(s',a')\n",
    "        next_Q = self.target_net(next_state)[np.arange(0, self.batch_size), best_action]\n",
    "\n",
    "        return ( reward + (1 - done.float()) * self.gamma * next_Q ).float()\n",
    "    \n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "    \n",
    "    def sync_Q_target(self):\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "    \n",
    "    def save(self):\n",
    "        save_path = (\n",
    "            self.save_dir/f\"mario_net_{int(self.curr_step)//self.save_every}.chkpt\"\n",
    "        )\n",
    "\n",
    "        torch.save(\n",
    "            dict(online_model=self.online_net.state_dict(),\n",
    "                target_model=self.target_net.state_dict(),\n",
    "                exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioOnlineNet(nn.Module):\n",
    "    \"\"\"\n",
    "        CNN structure:\n",
    "            input -> (conv2d+relu)*3 -> flatten -> (dense + relu)*2 -> output\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h!=84:\n",
    "            raise ValueError(f\"Expecting input height: 84 got: {h}\")\n",
    "        \n",
    "        if w!=84:\n",
    "            raise ValueError(f\"Expecting input width: 84 got: {w}\")\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=3136, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512,out_features=output_dim),\n",
    "        )\n",
    "\n",
    "               \n",
    "    def forward(self, input):\n",
    "        return self.net(input)\n",
    "\n",
    "class MarioTargetNet(nn.Module):\n",
    "    \"\"\"\n",
    "        CNN structure:\n",
    "            input -> (conv2d+relu)*3 -> flatten -> (dense + relu)*2 -> output\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h!=84:\n",
    "            raise ValueError(f\"Expecting input height: 84 got: {h}\")\n",
    "        \n",
    "        if w!=84:\n",
    "            raise ValueError(f\"Expecting input width: 84 got: {w}\")\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=3136, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512,out_features=output_dim),\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        # Q-target parameters are frozen\n",
    "        for p in self.net.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "    def forward(self, input, model=\"online\"):            \n",
    "        return self.net(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir/\"log\"\n",
    "\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        \n",
    "        # saving the plots\n",
    "        self.ep_rewards_plot = save_dir/\"reward_plot.png\"\n",
    "        self.ep_lengths_plot = save_dir/\"length_plot.png\"\n",
    "        self.ep_avg_losses_plot = save_dir/\"loss_plot.png\"\n",
    "        self.ep_avg_qs_plot = save_dir/\"q_plot.png\"\n",
    "\n",
    "        # history metircs\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # moving averages added for every call to record\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1 \n",
    "\n",
    "    def log_episode(self):\n",
    "        \"\"\"\n",
    "            marking the end of the episode\n",
    "        \"\"\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        \n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss/self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q/self.curr_ep_loss_length, 5)\n",
    "        \n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "    \n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "    \n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "        \n",
    "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\preet\\.conda\\envs\\mldl\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n",
      "C:\\Users\\preet\\AppData\\Local\\Temp/ipykernel_4984/1849942595.py:95: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  done = torch.tensor([done]).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Step 1042 - Epsilon 0.9742860568148791 - Mean Reward 2278.0 - Mean Length 1042.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 17.0 - Time 2022-05-24T00:39:35\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 10000\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "        env.render()\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d659314f07273798f94fa7ea2dd05839101ec9cddf4700630f4176b4de9856b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('mldl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
